{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACROBOT ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.3.1\n",
      "  Downloading matplotlib-3.3.1-cp38-cp38-macosx_10_9_x86_64.whl (8.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.5 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-1.7.1-cp38-none-macosx_10_9_x86_64.whl (108.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 108.9 MB 4.8 MB/s eta 0:00:01    |████████▍                       | 28.4 MB 441 kB/s eta 0:03:03     |███████████████████████████     | 91.8 MB 2.5 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting gym==0.17.3\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.50.2\n",
      "  Downloading tqdm-4.50.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.19.1\n",
      "  Downloading numpy-1.19.1-cp38-cp38-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements_acrobot.txt (line 1)) (2.4.7)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 212 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2020.06.20 in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements_acrobot.txt (line 1)) (2020.12.5)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.1.0-cp38-cp38-macosx_10_10_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements_acrobot.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from torch->-r requirements_acrobot.txt (line 2)) (3.7.4.3)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.6.0-cp38-cp38-macosx_10_9_x86_64.whl (30.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.7 MB 3.9 MB/s eta 0:00:011   |███████▌                        | 7.2 MB 4.0 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from gym==0.17.3->-r requirements_acrobot.txt (line 3)) (1.6.0)\n",
      "Requirement already satisfied: six in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.3.1->-r requirements_acrobot.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: future in /Users/tennismichel/opt/miniconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3->-r requirements_acrobot.txt (line 3)) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654652 sha256=e23c2e75bb2d5f3792a272c5e725dcec2587c38a963b7d8c88f6e2ae9550cc62\n",
      "  Stored in directory: /Users/tennismichel/Library/Caches/pip/wheels/84/40/e7/14efb9870cfc92ac236d78cb721dce614ddec9666c8a5e0a35\n",
      "Successfully built gym\n",
      "Installing collected packages: cycler, kiwisolver, numpy, pillow, matplotlib, torch, scipy, pyglet, gym, tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.51.0\n",
      "    Uninstalling tqdm-4.51.0:\n",
      "      Successfully uninstalled tqdm-4.51.0\n",
      "Successfully installed cycler-0.10.0 gym-0.17.3 kiwisolver-1.3.1 matplotlib-3.3.1 numpy-1.19.1 pillow-8.1.0 pyglet-1.5.0 scipy-1.6.0 torch-1.7.1 tqdm-4.50.2\n"
     ]
    }
   ],
   "source": [
    "# Libraries will not be installed if running on ifi-europa.uibk.ac.at\n",
    "\n",
    "# Make sure that the required libraries are installed on your local system\n",
    "# If you are using Google Colab, remember to upload the requirements file before \n",
    "# running this cell\n",
    "# If you are running this notebook locally, the requirements file needs to be in \n",
    "# the same location as this notebook\n",
    "import os\n",
    "running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n",
    "    \n",
    "if running_local:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install -r requirements_acrobot.txt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f5d51ebd4577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "from importlib import reload\n",
    "\n",
    "# from networks import NNetwork, NeuralNetworkPolicy\n",
    "from agents import Q_Agent, Q_DQN_Agent, SARSA_Agent, SARSA_DQN_Agent\n",
    "from agents_nnp import AAC_Agent, MC_PolGrad_Agent\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot is a 2-link pendulum with only the second joint actuated.\n",
    "Initially, both links point downwards. The goal is to swing the\n",
    "end-effector at a height at least the length of one link above the base.\n",
    "Both links can swing freely and can pass by each other, i.e., they don't\n",
    "collide when they have the same angle.\n",
    "\n",
    "**STATE:**\n",
    "The state consists of the sin() and cos() of the two rotational joint\n",
    "angles and the joint angular velocities :\n",
    "[cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].\n",
    "For the first link, an angle of 0 corresponds to the link pointing downwards.\n",
    "The angle of the second link is relative to the angle of the first link.\n",
    "An angle of 0 corresponds to having the same angle between the two links.\n",
    "A state of [1, 0, 1, 0, ..., ...] means that both links point downwards.\n",
    "\n",
    "**ACTIONS:**\n",
    "The action is either applying +1, 0 or -1 torque on the joint between\n",
    "the two pendulum links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "# Set a seed for reproducing results\n",
    "# random_seed = 1234\n",
    "# np.random.seed(random_seed)\n",
    "# torch.manual_seed(random_seed)\n",
    "# env.seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the type and size of the action space\n",
    "print(\"Action space: {}\".format(env.action_space))  # 3 discrete actions, \n",
    "\n",
    "# What does an action look like\n",
    "sample_action = env.action_space.sample()  # Action can be one of these: 0, 1, 2\n",
    "print(\"Sample action: {}\".format(sample_action))  # Execute multiple times to see different actions\n",
    "print(\"Type of action: {}\".format(type(sample_action)))\n",
    "\n",
    "# What is the type and size of the observation (state) space\n",
    "print(\"Observationtate space: {}\".format(env.observation_space))  # continuous states\n",
    "\n",
    "# Which state does the agent start in?\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state: {}\".format(initial_state))  \n",
    "\n",
    "# What is an observation\n",
    "sample_observation = env.observation_space.sample()\n",
    "print(\"Sample observation: {}\".format(sample_observation))\n",
    "print(\"Type of observation: {}\".format(type(sample_observation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TRAINING####\n",
    "training_results = list() # A list for storing the hyperparameters and the corresponding results\n",
    "max_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train AAC-Agent (neural network policy - agent) ###\n",
    "hyperparam_dict = {'name': 'AAC-Agent', 'learning_rate':0.01, 'gamma':0.99}\n",
    "\n",
    "aac_agent = AAC_Agent(env, num_episodes=max_episodes, num_steps=500, learning_rate=0.001,\n",
    "                      gamma=0.99, hidden_dim=100, dropout=0.6, log_interval=5)\n",
    "ep_rewards, running_rewards = aac_agent.train()\n",
    "training_results.append((hyperparam_dict, ep_rewards, running_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Monte Carlo policy gradient Agent (REINFORCE - Agent) ###\n",
    "hyperparam_dict = {'name': 'MC_PolGrad-Agent', 'learning_rate':0.001, 'gamma':0.99}\n",
    "mc_polGrad_agent = MC_PolGrad_Agent(env, num_episodes=max_episodes, num_steps=500, learning_rate=0.001,\n",
    "                      gamma=0.99, hidden_dim=100, dropout=0.6, log_interval=5)\n",
    "ep_rewards, running_rewards = mc_polGrad_agent.train()\n",
    "training_results.append((hyperparam_dict, ep_rewards, running_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train SARSA-Agent (semi-gradient) ###\n",
    "hyperparam_dict = {'name': 'SARSA-Agent', 'learning_rate':0.0002, 'gamma':0.99, 'epsilon':0.1}\n",
    "sarsa_agent = SARSA_Agent(env, num_episodes=max_episodes, num_steps=500, learning_rate=0.0001,\n",
    "                          gamma=0.99, epsilon=0.2, n_hidden_neurons=16, log_interval=5)\n",
    "ep_rewards, running_rewards = sarsa_agent.train()\n",
    "training_results.append((hyperparam_dict, ep_rewards, running_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Q-Agent (semi-gradient) ###\n",
    "hyperparam_dict = {'name': 'Q-Agent', 'learning_rate':0.0001, 'gamma':0.99, 'epsilon':0.1}\n",
    "q_agent = Q_Agent(env, num_episodes=max_episodes, num_steps=500, learning_rate=0.0001,\n",
    "                  gamma=0.99, epsilon=0.1, n_hidden_neurons=200, log_interval=100)\n",
    "ep_rewards, running_rewards = q_agent.train()\n",
    "training_results.append((hyperparam_dict, ep_rewards, running_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Q_DQN-Agent (semi-gradient) ###\n",
    "# neurons=16 and epsilon=0.2\n",
    "hyperparam_dict = {'name': 'Q-DQN-Agent', 'learning_rate':0.0001, 'gamma':0.99, 'epsilon':0.1}\n",
    "q_dqn_agent = Q_DQN_Agent(env, num_episodes=max_episodes, num_steps=500, learning_rate=0.0001,\n",
    "                  gamma=0.99, epsilon=0.2, n_hidden_neurons=16, log_interval=5)\n",
    "ep_rewards, running_rewards = q_dqn_agent.train()\n",
    "training_results.append((hyperparam_dict, ep_rewards, running_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train SARSA_DQN-Agent (semi-gradient) ###\n",
    "# neurons=16 and epsilon=0.2\n",
    "hyperparam_dict = {'name': 'SARSA-DQN-Agent', 'learning_rate':0.0001, 'gamma':0.99, 'epsilon':0.1}\n",
    "sarsa_dqn_agent = SARSA_DQN_Agent(env, num_episodes=max_episodes, num_steps=500, learning_rate=0.0001,\n",
    "                  gamma=0.99, epsilon=0.2, n_hidden_neurons=16, log_interval=5)\n",
    "ep_rewards, running_rewards = sarsa_dqn_agent.train()\n",
    "training_results.append((hyperparam_dict, ep_rewards, running_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(1, figsize=(20,8))\n",
    "\n",
    "for result in training_results:\n",
    "    hp = result[0]\n",
    "    ep_rewards = result[1]\n",
    "    running_rewards = result[2]\n",
    "    # plt.plot(range(len(ep_rewards)), ep_rewards, lw=2, color=\"red\", label=hp['name'])\n",
    "    plt.plot(range(len(running_rewards)), running_rewards, lw=2, label=hp['name'])\n",
    "    \n",
    "    # title_str = hp['name'] + '($\\gamma$:' + str(hp['gamma']) + ',lr:' + str(hp['learning_rate']) + ')'\n",
    "    title_str = \"Acrobot-v1 ($n_{hidden}$: 16, $\\gamma$: 0.99)\"\n",
    "    plt.title(title_str)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Running average of Rewards')\n",
    "plt.legend() # ncol=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = sarsa_agent.evaluation(n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = q_dqn_agent.evaluation(n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####VIDEO#####\n",
    "env_to_wrap = gym.make('Acrobot-v1')\n",
    "env = gym.wrappers.Monitor(env_to_wrap, 'videos/AAC', force = True)\n",
    "ep_rewards = aac_agent.polGrad_evaluation(n_episodes=10, vid_env=env)\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_to_wrap = gym.make('Acrobot-v1')\n",
    "env = gym.wrappers.Monitor(env_to_wrap, 'videos/SARSA', force = True)\n",
    "ep_rewards = sarsa_agent.evaluation(n_episodes=3, vid_env=env)\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-advml",
   "language": "python",
   "name": "venv-advml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
