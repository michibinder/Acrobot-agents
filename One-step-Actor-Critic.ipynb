{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.3.1 in /home/c7031297/FNN/lib/python3.6/site-packages (from -r requirements_04_rl_pol_grad.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: torch in /home/c7031297/FNN/lib/python3.6/site-packages (from -r requirements_04_rl_pol_grad.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: gym==0.17.3 in /home/c7031297/FNN/lib/python3.6/site-packages (from -r requirements_04_rl_pol_grad.txt (line 3)) (0.17.3)\n",
      "Requirement already satisfied: tqdm==4.50.2 in /home/c7031297/FNN/lib/python3.6/site-packages (from -r requirements_04_rl_pol_grad.txt (line 4)) (4.50.2)\n",
      "Requirement already satisfied: numpy==1.19.1 in /home/c7031297/FNN/lib/python3.6/site-packages (from -r requirements_04_rl_pol_grad.txt (line 5)) (1.19.1)\n",
      "Requirement already satisfied: torchsummary==1.5.1 in /home/c7031297/FNN/lib/python3.6/site-packages (from -r requirements_04_rl_pol_grad.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3/dist-packages (from matplotlib==3.3.1->-r requirements_04_rl_pol_grad.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/c7031297/FNN/lib/python3.6/site-packages (from matplotlib==3.3.1->-r requirements_04_rl_pol_grad.txt (line 1)) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/lib/python3/dist-packages (from matplotlib==3.3.1->-r requirements_04_rl_pol_grad.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib==3.3.1->-r requirements_04_rl_pol_grad.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/c7031297/FNN/lib/python3.6/site-packages (from matplotlib==3.3.1->-r requirements_04_rl_pol_grad.txt (line 1)) (2020.11.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/c7031297/FNN/lib/python3.6/site-packages (from matplotlib==3.3.1->-r requirements_04_rl_pol_grad.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions in /home/c7031297/FNN/lib/python3.6/site-packages (from torch->-r requirements_04_rl_pol_grad.txt (line 2)) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/c7031297/FNN/lib/python3.6/site-packages (from torch->-r requirements_04_rl_pol_grad.txt (line 2)) (0.8)\n",
      "Requirement already satisfied: scipy in /home/c7031297/.local/lib/python3.6/site-packages (from gym==0.17.3->-r requirements_04_rl_pol_grad.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/c7031297/.local/lib/python3.6/site-packages (from gym==0.17.3->-r requirements_04_rl_pol_grad.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/c7031297/.local/lib/python3.6/site-packages (from gym==0.17.3->-r requirements_04_rl_pol_grad.txt (line 3)) (1.4.8)\n",
      "Requirement already satisfied: future in /home/c7031297/.local/lib/python3.6/site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3->-r requirements_04_rl_pol_grad.txt (line 3)) (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/c7031297/FNN/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Always execute this cell\n",
    "\n",
    "# Libraries will not be installed if running on ifi-europa.uibk.ac.at\n",
    "\n",
    "# Make sure that the required libraries are installed on your local system\n",
    "# If you are using Google Colab, remember to upload the requirements file before \n",
    "# running this cell\n",
    "# If you are running this notebook locally, the requirements file needs to be in \n",
    "# the same location as this notebook\n",
    "import os\n",
    "running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n",
    "    \n",
    "if running_local:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install -r requirements_04_rl_pol_grad.txt\n",
    "\n",
    "#     !{sys.executable} -m pip install -r requirements_04_rl_qlearn.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fadf459ef48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set a seed for reproducing results\n",
    "random_seed = 1234\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Sample action: 0\n",
      "Type of action: <class 'int'>\n",
      "Observationtate space: Box(-28.274333953857422, 28.274333953857422, (6,), float32)\n",
      "Initial state: [ 0.99965389 -0.02630794  0.99678118 -0.08017036  0.0960371  -0.00889859]\n",
      "Sample observation: [ -0.6874997    0.5421291   -0.3066365    0.07097009   6.284189\n",
      " -13.656844  ]\n",
      "Type of observation: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Define the environment\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('Acrobot-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# Set the seed for gym\n",
    "env.seed(random_seed)\n",
    "\n",
    "# What is the type and size of the action space\n",
    "print(\"Action space: {}\".format(env.action_space))  # 2 discrete actions, \n",
    "\n",
    "# What does an action look like\n",
    "sample_action = env.action_space.sample()  # Action can be one of these: 0, 1\n",
    "print(\"Sample action: {}\".format(sample_action))  # Execute multiple times to see different actions\n",
    "print(\"Type of action: {}\".format(type(sample_action)))\n",
    "\n",
    "# What is the type and size of the observation (state) space\n",
    "print(\"Observationtate space: {}\".format(env.observation_space))  # continuous states\n",
    "\n",
    "# Which state does the agent start in?\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state: {}\".format(initial_state))  \n",
    "\n",
    "# What is an observation\n",
    "sample_observation = env.observation_space.sample()\n",
    "print(\"Sample observation: {}\".format(sample_observation))\n",
    "print(\"Type of observation: {}\".format(type(sample_observation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super(Critic_Network, self).__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.Num_states = env.observation_space.shape[0]\n",
    "        self.Num_actions = env.action_space.n\n",
    "        print(\"States and actions=\", self.Num_states, self.Num_actions)\n",
    "        \n",
    "\n",
    "        \n",
    "        NumNeurons=64\n",
    "        self.affine1 = nn.Linear(self.Num_states,NumNeurons)\n",
    "        self.affine2 = nn.Linear(NumNeurons, NumNeurons)\n",
    "        self.affine5 = nn.Linear(NumNeurons, 1)\n",
    "        # Used for storing the log probabilities of the actions\n",
    "        # which is required to compute the loss (and hence needed for the parameter update step)\n",
    "        self.saved_log_probs = []\n",
    "        #List to save critic value\n",
    "        self.critic_value_history = []\n",
    "        \n",
    "        # Used for tracking the rewards the agent recieves in an episode.\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the policy network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The current state as observed by the agent.\n",
    "        Returns:\n",
    "            (Tensor): (Actor,Critic_value)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        #Commun Layers\n",
    "        out=F.relu(self.affine1(x))\n",
    "        out=F.relu(self.affine2(out))\n",
    "        out=self.affine5(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def Calulcate_value(self,state):\n",
    "        \"\"\"\n",
    "        *Selects an action for the agent, by sampling from the action probabilities\n",
    "        produced by the network, based on the current state. \n",
    "        *stores the log probability of the actions.\n",
    "        *Stores the critic value\n",
    "        \n",
    "        Args:\n",
    "            state (numpy array): The current state as observed by the agent.\n",
    "            \n",
    "        Returns:\n",
    "            (int): Action to perform.\n",
    "        \"\"\"\n",
    "        # Convert the state from a numpy array to a torch tensor\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        #GET THE ACTION\n",
    "        V = self.forward(state)\n",
    "        return V\n",
    "\n",
    "    def save(self, state_file='Critic_network.pt', save_dir='models'):\n",
    "        \"\"\"\n",
    "        Saves a trained policy network.\n",
    "        \"\"\"\n",
    "        # Save the model state\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(save_dir, state_file))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(state_file='models/Critic_Network.pt'):\n",
    "        \"\"\"\n",
    "        Loads a trained Critic network.\n",
    "        \"\"\"\n",
    "        # Create a network object with the constructor parameters\n",
    "        Critic = Critic_Network()\n",
    "        # Load the weights\n",
    "        Critic.load_state_dict(torch.load(state_file))\n",
    "        # Set the network to evaluation mode\n",
    "        Critic.eval()\n",
    "        return Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkPolicy(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super(NeuralNetworkPolicy, self).__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.Num_states = env.observation_space.shape[0]\n",
    "        self.Num_actions = env.action_space.n\n",
    "        print(\"States and actions=\", self.Num_states, self.Num_actions)\n",
    "        \n",
    "\n",
    "        #Layer definitions\n",
    "        NumNeurons=64\n",
    "        self.affine1 = nn.Linear(self.Num_states,NumNeurons)\n",
    "        self.affine2 = nn.Linear(NumNeurons, NumNeurons)\n",
    "        self.affine5 = nn.Linear(NumNeurons, self.Num_actions)\n",
    "        \n",
    "        # Used for storing the log probabilities of the actions\n",
    "        # which is required to compute the loss (and hence needed for the parameter update step)\n",
    "        self.saved_log_probs = []\n",
    "        # Used for tracking the rewards the agent recieves in an episode.\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the policy network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The current state as observed by the agent.\n",
    "        Returns:\n",
    "            (Tensor): (Actor,Critic_value)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        #Commun Layers\n",
    "        out=F.relu(self.affine1(x))\n",
    "        out=F.relu(self.affine2(out))\n",
    "        out=F.relu(self.affine5(out))\n",
    "        out=F.softmax(out,dim=-1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    def select_action(self,state):\n",
    "        \"\"\"\n",
    "        *Selects an action for the agent, by sampling from the action probabilities\n",
    "        produced by the network, based on the current state. \n",
    "        *stores the log probability of the actions.\n",
    "        *Stores the critic value\n",
    "        \n",
    "        Args:\n",
    "            state (numpy array): The current state as observed by the agent.\n",
    "            \n",
    "        Returns:\n",
    "            (int): Action to perform.\n",
    "        \"\"\"\n",
    "        # Convert the state from a numpy array to a torch tensor\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        #GET THE ACTION\n",
    "        # Get the predicted probabilities from the policy network\n",
    "        probs = self.forward(state)\n",
    "        # Sample the actions according to their respective probabilities\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        # Also calculate the log of the probability for the selected action\n",
    "        logProb=m.log_prob(action)\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        \n",
    "\n",
    "        # Return the chosen action\n",
    "        return action.item(),logProb\n",
    "\n",
    "    def save(self, state_file='policy_network.pt', save_dir='models'):\n",
    "        \"\"\"\n",
    "        Saves a trained policy network.\n",
    "        \"\"\"\n",
    "        # Save the model state\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(save_dir, state_file))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(state_file='models/policy_network.pt'):\n",
    "        \"\"\"\n",
    "        Loads a trained policy network.\n",
    "        \"\"\"\n",
    "        # Create a network object with the constructor parameters\n",
    "        policy = NeuralNetworkPolicy()\n",
    "        # Load the weights\n",
    "        policy.load_state_dict(torch.load(state_file))\n",
    "        # Set the network to evaluation mode\n",
    "        policy.eval()\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic:\n",
    "    def __init__(self,env=env,log_interval=5, max_episodes=1000, T=1000, save=False, **hyperparam_dict):\n",
    "          \n",
    "        \"\"\"\n",
    "        Loading hyperparameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch the hyperparameters\n",
    "        self.gamma = hyperparam_dict['gamma']\n",
    "        self.learning_rate = hyperparam_dict['learning_rate']\n",
    "\n",
    "        state_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Create the policy function and set the training mode\n",
    "        self.policy = NeuralNetworkPolicy(env=env)\n",
    "        self.policy.train()\n",
    "\n",
    "        # Define the optimizer and set the learning rate\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.learning_rate)\n",
    "        self.T=T\n",
    "        self.log_interval=log_interval\n",
    "        self.max_episodes=max_episodes\n",
    "        self.save=save\n",
    "        \n",
    "        #Critic Network\n",
    "        self.Value_Function = Critic_Network(env=env)\n",
    "        self.Value_Function.train()\n",
    "        self.optimizer_Critic = optim.Adam(self.Value_Function.parameters(), lr=self.learning_rate)\n",
    "        self.Critic_loss=nn.MSELoss()\n",
    "        \n",
    "        # Lists to store the episodic and running rewards for plotting\n",
    "        self.ep_rewards = list()\n",
    "        self.running_rewards = list()\n",
    "        self.Bandera=False\n",
    "    \n",
    "    def Reinforce(self):\n",
    "          \n",
    "\n",
    "        \"\"\"\n",
    "        Implementation of the main body of the ACTOR-CRITIC algorithm.\n",
    "\n",
    "        Args:\n",
    "            policy (NeuralNetworkPolicy): The policy neural network.\n",
    "            optimizer (child of torch.optim.Optimizer): Optimizer algorithm for gradient ascent.\n",
    "            gamma (float): Discount factor in the range [0.0,1.0]. Defaults to 0.9.\n",
    "            log_interval (int): Prints the progress after this many episodes. Defaults to 100.\n",
    "            max_episodes (int): Maximum number of episodes to train for. Defaults to 1000.\n",
    "            save (bool): Whether to save the trained network. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            ep_rewards (list): List of actual cumulative rewards in each episode. \n",
    "            running_rewards (numpy array): List of smoothed cumulative rewards in each episode. \n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        # Start executing an episode\n",
    "        for i_episode in count(1):\n",
    "            # Steps you need to implement\n",
    "            # 1. Reset the environment\n",
    "            current_state = env.reset()\n",
    "            # 2. Initialize `ep_reward` (the total reward for this episode)\n",
    "            ep_reward =0\n",
    "            # 3. For each step of the episode\n",
    "            done=False\n",
    "            t=0\n",
    "            I=1\n",
    "            while (not done):\n",
    "                \n",
    "                \n",
    "                # 3.1 Select an action using the policy network\n",
    "                action,logProb = self.policy.select_action(current_state)\n",
    "                # 3.2 Perform the action and note the next state and reward and if the episode is done\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Calculate Current and next value with Critic Network\n",
    "                V_current=self.Value_Function.Calulcate_value(current_state)\n",
    "                V_next=self.Value_Function.Calulcate_value(next_state)\n",
    "                #V_next is 0 if the state is terminal\n",
    "                if done==True:\n",
    "                    V_next=torch.tensor([[0]])\n",
    "                    \n",
    "                #TD Target\n",
    "                y_t=reward+self.gamma*V_next\n",
    "                I=pow(self.gamma,I)\n",
    "                \n",
    "                # Reset the gradients of the parameters\n",
    "                self.optimizer.zero_grad()\n",
    "                self.optimizer_Critic.zero_grad()\n",
    "\n",
    "                #Critic minimizing loss\n",
    "                delta_t=self.Critic_loss(y_t,V_current)\n",
    "\n",
    "                # Compute the cumulative loss\n",
    "                Critic_loss_ = delta_t\n",
    "                policy_loss = -logProb*I*delta_t\n",
    "\n",
    "                # Backpropagate the loss through the network\n",
    "                policy_loss.backward(retain_graph=True)\n",
    "                Critic_loss_.backward()#retain_graph=True)\n",
    "                \n",
    "                # Perform a parameter update step\n",
    "                self.optimizer_Critic.step()                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                #Update state\n",
    "                current_state=next_state\n",
    "                \n",
    "                # 3.3 Store the current reward in `policy.rewards`\n",
    "                self.policy.rewards.append(reward)\n",
    "\n",
    "                # 3.4 Increment the total reward in this episode\n",
    "                ep_reward+=reward\n",
    "                \n",
    "                # 3.5 Check if the episode is finished using the `done` variable and break if yes\n",
    "                t+=1\n",
    "                if done==True or t>self.T:\n",
    "                    break\n",
    "            \n",
    "            if i_episode==1:\n",
    "                # To track the reward across consecutive episodes (smoothed)\n",
    "                running_reward = ep_reward\n",
    "                \n",
    "            # Update the running reward\n",
    "            running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "            # Store the rewards for plotting\n",
    "            self.ep_rewards.append(ep_reward)\n",
    "            self.running_rewards.append(running_reward)\n",
    "\n",
    "\n",
    "            if i_episode % self.log_interval == 0:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                      i_episode, ep_reward, running_reward))\n",
    "\n",
    "            if i_episode >= self.max_episodes:\n",
    "                print('Max episodes exceeded, quitting.')\n",
    "                break\n",
    "        # Save the trained policy network\n",
    "        if self.save:\n",
    "            self.policy.save()\n",
    "        \n",
    "\n",
    "    \n",
    "    def Plot(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Plot the Episode Rewards and Running Rewards\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "        # Plot the results\n",
    "        fig = plt.figure(1, figsize=(20,8))\n",
    "        title_str = \"Neural_Network\" + '($\\gamma$:' + str(self.gamma) + ',lr:' + str(self.learning_rate) + ')'\n",
    "        plt.plot(range(len(self.ep_rewards)), self.ep_rewards, lw=2, color=\"red\", label=\"episode rewards\")\n",
    "        plt.plot(range(len(self.running_rewards)), self.running_rewards, lw=2, color=\"blue\", label=\"running rewards\")\n",
    "        plt.title(title_str)\n",
    "\n",
    "        plt.grid()\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Running average of Rewards')\n",
    "        plt.legend(ncol=1)\n",
    "        plt.show()\n",
    "        \n",
    "    def Test_Environment(self):\n",
    "        \"Test the trained Agent in the environment\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Steps you need to implement\n",
    "        # 1. Reset the environment\n",
    "        current_state = env.reset()\n",
    "        # 3. For each step of the episode\n",
    "        for i in range(self.max_episodes):\n",
    "            # 3.1 Select an action using the policy network\n",
    "            action,logProb = self.policy.select_action(current_state)\n",
    "            # 3.2 Perform the action and note the next state and reward and if the episode is done\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if i%1==0:\n",
    "                env.render()\n",
    "            current_state=next_state\n",
    "            # 3.5 Check if the episode is finished using the `done` variable and break if yes\n",
    "            time.sleep(0.05)\n",
    "            if done==True:\n",
    "                \n",
    "                \n",
    "                print(\"i=\",i)\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        time.sleep(2)\n",
    "        env.close()\n",
    "    def Delete(self):\n",
    "        del self.optimizer\n",
    "        del self.optimizer_Critic\n",
    "        del self.policy\n",
    "        del self.Value_Function\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States and actions= 6 3\n",
      "States and actions= 6 3\n",
      "SUMMARY ACTOR NETWORK\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             448\n",
      "            Linear-2                   [-1, 64]           4,160\n",
      "            Linear-3                    [-1, 3]             195\n",
      "================================================================\n",
      "Total params: 4,803\n",
      "Trainable params: 4,803\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "SUMMARY CRITIC NETWORK\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             448\n",
      "            Linear-2                   [-1, 64]           4,160\n",
      "            Linear-3                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 4,673\n",
      "Trainable params: 4,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test cell: Here we will just test that all the functions execute without error\n",
    "# Agent.Delete()\n",
    "# Run the REINFORCE algorithm \n",
    "hyperparam_dict = {'name': 'neural_network', 'gamma':0.99, 'learning_rate':0.001}\n",
    "\n",
    "# hyperparam_dict = {'name': 'neural_network', 'gamma':0.1, 'learning_rate':0.002}\n",
    "\n",
    "Agent=Actor_Critic(env=env,log_interval=5,max_episodes=200,T=1000, **hyperparam_dict)\n",
    "\n",
    "# Check the network structure of the q-network\n",
    "print(\"SUMMARY ACTOR NETWORK\")\n",
    "print(summary(Agent.policy, input_size=env.observation_space.shape))\n",
    "print(\"SUMMARY CRITIC NETWORK\")\n",
    "print(summary(Agent.Value_Function, input_size=env.observation_space.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 10\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 15\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 20\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 25\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 30\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 35\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 40\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 45\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 50\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 55\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 60\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 65\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 70\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 75\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 80\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 85\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 90\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 95\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 100\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 105\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 110\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 115\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 120\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 125\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 130\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 135\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 140\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 145\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 150\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 155\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 160\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 165\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 170\tLast reward: -500.00\tAverage reward: -500.00\n",
      "Episode 175\tLast reward: -500.00\tAverage reward: -500.00\n"
     ]
    }
   ],
   "source": [
    "Agent.Reinforce()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent.Plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent.Test_Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent.Delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNN",
   "language": "python",
   "name": "fnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
